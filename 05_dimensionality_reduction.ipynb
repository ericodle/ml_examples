{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensionality Reduction - PCA and t-SNE Complete Guide\n",
        "\n",
        "## Table of Contents\n",
        "1. [What is Dimensionality Reduction?](#what-is-dr)\n",
        "2. [Principal Component Analysis (PCA)](#pca)\n",
        "3. [t-Distributed Stochastic Neighbor Embedding (t-SNE)](#tsne)\n",
        "4. [Visual Demonstrations](#visual-demos)\n",
        "5. [Practical Examples](#practical-examples)\n",
        "6. [PCA vs t-SNE Comparison](#comparison)\n",
        "7. [Advanced Techniques](#advanced)\n",
        "8. [Summary and Key Takeaways](#summary)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is Dimensionality Reduction? {#what-is-dr}\n",
        "\n",
        "**Dimensionality reduction** is the process of reducing the number of features (dimensions) in a dataset while preserving the most important information. It's like taking a high-resolution photo and creating a lower-resolution version that still shows the main details.\n",
        "\n",
        "### Why Do We Need It?\n",
        "\n",
        "#### **The Curse of Dimensionality:**\n",
        "- **High-dimensional data** becomes sparse and hard to visualize\n",
        "- **Computational complexity** increases exponentially\n",
        "- **Overfitting** becomes more likely\n",
        "- **Distance metrics** become less meaningful\n",
        "\n",
        "#### **Benefits of Dimensionality Reduction:**\n",
        "- **Visualization** - See data in 2D/3D\n",
        "- **Noise reduction** - Remove less important features\n",
        "- **Computational efficiency** - Faster algorithms\n",
        "- **Storage** - Less memory needed\n",
        "- **Feature extraction** - Find meaningful patterns\n",
        "\n",
        "### Types of Dimensionality Reduction:\n",
        "\n",
        "#### **Linear Methods:**\n",
        "- **PCA** - Principal Component Analysis\n",
        "- **LDA** - Linear Discriminant Analysis\n",
        "- **ICA** - Independent Component Analysis\n",
        "\n",
        "#### **Non-linear Methods:**\n",
        "- **t-SNE** - t-Distributed Stochastic Neighbor Embedding\n",
        "- **UMAP** - Uniform Manifold Approximation and Projection\n",
        "- **Isomap** - Isometric Mapping\n",
        "- **LLE** - Locally Linear Embedding\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Principal Component Analysis (PCA) {#pca}\n",
        "\n",
        "PCA is the most popular linear dimensionality reduction technique. It finds the directions (principal components) where the data varies the most.\n",
        "\n",
        "### How PCA Works:\n",
        "\n",
        "#### **Step 1: Center the Data**\n",
        "- Subtract the mean from each feature\n",
        "- This centers the data around the origin\n",
        "\n",
        "#### **Step 2: Calculate Covariance Matrix**\n",
        "- Shows how features vary together\n",
        "- Diagonal elements = variance of each feature\n",
        "- Off-diagonal elements = covariance between features\n",
        "\n",
        "#### **Step 3: Find Eigenvectors and Eigenvalues**\n",
        "- **Eigenvectors** = principal components (directions of maximum variance)\n",
        "- **Eigenvalues** = amount of variance explained by each component\n",
        "\n",
        "#### **Step 4: Project Data**\n",
        "- Transform data to new coordinate system\n",
        "- Choose top k components that explain most variance\n",
        "\n",
        "### Mathematical Foundation:\n",
        "\n",
        "The first principal component is the direction that maximizes:\n",
        "```\n",
        "variance = w^T * C * w\n",
        "```\n",
        "Where:\n",
        "- `w` = unit vector (principal component)\n",
        "- `C` = covariance matrix\n",
        "- `w^T * w = 1` (unit length constraint)\n",
        "\n",
        "### Key Properties:\n",
        "- **Orthogonal** - Principal components are perpendicular\n",
        "- **Ordered** - First component explains most variance\n",
        "- **Linear** - Preserves linear relationships\n",
        "- **Reversible** - Can reconstruct original data (with some loss)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. t-Distributed Stochastic Neighbor Embedding (t-SNE) {#tsne}\n",
        "\n",
        "t-SNE is a non-linear dimensionality reduction technique that's excellent for visualization. It focuses on preserving local neighborhood structure.\n",
        "\n",
        "### How t-SNE Works:\n",
        "\n",
        "#### **Step 1: Calculate Pairwise Similarities**\n",
        "- **High-dimensional space**: Use Gaussian distribution\n",
        "- **Low-dimensional space**: Use t-distribution (heavier tails)\n",
        "\n",
        "#### **Step 2: Optimize Low-Dimensional Embedding**\n",
        "- Minimize difference between high-dim and low-dim similarities\n",
        "- Use gradient descent to find optimal positions\n",
        "\n",
        "#### **Step 3: Preserve Local Structure**\n",
        "- Points that are close in high-dim stay close in low-dim\n",
        "- Points that are far apart can be far apart in low-dim\n",
        "\n",
        "### Key Properties:\n",
        "- **Non-linear** - Can capture complex patterns\n",
        "- **Local focus** - Preserves neighborhood structure\n",
        "- **Non-reversible** - Cannot reconstruct original data\n",
        "- **Stochastic** - Results can vary between runs\n",
        "- **Computationally expensive** - Slower than PCA\n",
        "\n",
        "### Parameters:\n",
        "- **Perplexity** - Controls neighborhood size (5-50, typically 30)\n",
        "- **Learning rate** - How fast to learn (100-1000, typically 200)\n",
        "- **Number of iterations** - How long to optimize (1000+)\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Visual Demonstrations {#visual-demos}\n",
        "\n",
        "Let's start by importing the necessary libraries and creating some visualizations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(\"ðŸ“Š Ready to explore Dimensionality Reduction!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 PCA Demonstration with 2D Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 2D data to demonstrate PCA\n",
        "def create_2d_data():\n",
        "    \"\"\"Create 2D data with clear principal components\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create correlated 2D data\n",
        "    n_samples = 200\n",
        "    angle = np.pi / 4  # 45 degrees\n",
        "    \n",
        "    # Generate data along the main diagonal\n",
        "    x1 = np.random.normal(0, 2, n_samples)\n",
        "    x2 = x1 * np.sin(angle) + np.random.normal(0, 0.5, n_samples)\n",
        "    \n",
        "    # Rotate the data\n",
        "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
        "                               [np.sin(angle), np.cos(angle)]])\n",
        "    \n",
        "    data = np.column_stack([x1, x2])\n",
        "    rotated_data = data @ rotation_matrix.T\n",
        "    \n",
        "    return rotated_data\n",
        "\n",
        "# Create and visualize 2D data\n",
        "X_2d = create_2d_data()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Original data\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.6, s=50)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Original 2D Data')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "# Apply PCA\n",
        "pca_2d = PCA()\n",
        "X_pca_2d = pca_2d.fit_transform(X_2d)\n",
        "\n",
        "# Plot with principal components\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=50)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('Data After PCA')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Show explained variance\n",
        "explained_variance = pca_2d.explained_variance_ratio_\n",
        "print(f\"Explained variance ratio: {explained_variance}\")\n",
        "print(f\"First PC explains {explained_variance[0]:.1%} of variance\")\n",
        "print(f\"Second PC explains {explained_variance[1]:.1%} of variance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 PCA with Higher Dimensional Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Iris dataset (4D -> 2D)\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
        "print(f\"Features: {feature_names}\")\n",
        "\n",
        "# Apply PCA\n",
        "pca_iris = PCA(n_components=2)\n",
        "X_pca_iris = pca_iris.fit_transform(X_iris)\n",
        "\n",
        "# Visualize PCA results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Original data (first 2 features)\n",
        "plt.subplot(1, 3, 1)\n",
        "colors = ['red', 'green', 'blue']\n",
        "for i, color in enumerate(colors):\n",
        "    plt.scatter(X_iris[y_iris == i, 0], X_iris[y_iris == i, 1], \n",
        "                c=color, label=iris.target_names[i], alpha=0.7)\n",
        "plt.xlabel(feature_names[0])\n",
        "plt.ylabel(feature_names[1])\n",
        "plt.title('Original Data (First 2 Features)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# PCA results\n",
        "plt.subplot(1, 3, 2)\n",
        "for i, color in enumerate(colors):\n",
        "    plt.scatter(X_pca_iris[y_iris == i, 0], X_pca_iris[y_iris == i, 1], \n",
        "                c=color, label=iris.target_names[i], alpha=0.7)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA Results (2D)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Explained variance\n",
        "plt.subplot(1, 3, 3)\n",
        "explained_variance = pca_iris.explained_variance_ratio_\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance by Component')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Explained variance: {explained_variance}\")\n",
        "print(f\"Total variance explained: {explained_variance.sum():.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 t-SNE Demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply t-SNE to Iris dataset\n",
        "print(\"Applying t-SNE to Iris dataset...\")\n",
        "tsne_iris = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne_iris = tsne_iris.fit_transform(X_iris)\n",
        "\n",
        "# Compare PCA vs t-SNE\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# PCA results\n",
        "plt.subplot(1, 3, 1)\n",
        "for i, color in enumerate(colors):\n",
        "    plt.scatter(X_pca_iris[y_iris == i, 0], X_pca_iris[y_iris == i, 1], \n",
        "                c=color, label=iris.target_names[i], alpha=0.7)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA Results')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# t-SNE results\n",
        "plt.subplot(1, 3, 2)\n",
        "for i, color in enumerate(colors):\n",
        "    plt.scatter(X_tsne_iris[y_iris == i, 0], X_tsne_iris[y_iris == i, 1], \n",
        "                c=color, label=iris.target_names[i], alpha=0.7)\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE Results')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Side-by-side comparison\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(X_pca_iris[:, 0], X_pca_iris[:, 1], c=y_iris, alpha=0.7, s=50, label='PCA')\n",
        "plt.scatter(X_tsne_iris[:, 0], X_tsne_iris[:, 1], c=y_iris, alpha=0.7, s=50, marker='^', label='t-SNE')\n",
        "plt.xlabel('Component 1')\n",
        "plt.ylabel('Component 2')\n",
        "plt.title('PCA vs t-SNE Overlay')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… t-SNE visualization complete!\")\n",
        "print(\"ðŸ’¡ Notice how t-SNE creates tighter clusters than PCA\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Practical Examples {#practical-examples}\n",
        "\n",
        "### 5.1 Wine Dataset - High Dimensional Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Wine dataset (13D -> 2D)\n",
        "wine = datasets.load_wine()\n",
        "X_wine = wine.data\n",
        "y_wine = wine.target\n",
        "wine_feature_names = wine.feature_names\n",
        "\n",
        "print(f\"Wine dataset shape: {X_wine.shape}\")\n",
        "print(f\"Number of features: {len(wine_feature_names)}\")\n",
        "print(f\"Classes: {wine.target_names}\")\n",
        "\n",
        "# Standardize the data (important for PCA)\n",
        "scaler = StandardScaler()\n",
        "X_wine_scaled = scaler.fit_transform(X_wine)\n",
        "\n",
        "# Apply PCA\n",
        "pca_wine = PCA(n_components=2)\n",
        "X_pca_wine = pca_wine.fit_transform(X_wine_scaled)\n",
        "\n",
        "# Apply t-SNE\n",
        "print(\"Applying t-SNE to Wine dataset...\")\n",
        "tsne_wine = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "X_tsne_wine = tsne_wine.fit_transform(X_wine_scaled)\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# PCA results\n",
        "plt.subplot(1, 3, 1)\n",
        "wine_colors = ['red', 'green', 'blue']\n",
        "for i, color in enumerate(wine_colors):\n",
        "    plt.scatter(X_pca_wine[y_wine == i, 0], X_pca_wine[y_wine == i, 1], \n",
        "                c=color, label=wine.target_names[i], alpha=0.7)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('PCA Results (Wine Dataset)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# t-SNE results\n",
        "plt.subplot(1, 3, 2)\n",
        "for i, color in enumerate(wine_colors):\n",
        "    plt.scatter(X_tsne_wine[y_wine == i, 0], X_tsne_wine[y_wine == i, 1], \n",
        "                c=color, label=wine.target_names[i], alpha=0.7)\n",
        "plt.xlabel('t-SNE Component 1')\n",
        "plt.ylabel('t-SNE Component 2')\n",
        "plt.title('t-SNE Results (Wine Dataset)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Explained variance\n",
        "plt.subplot(1, 3, 3)\n",
        "explained_variance = pca_wine.explained_variance_ratio_\n",
        "plt.bar(range(1, len(explained_variance) + 1), explained_variance)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance (Wine)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"PCA explained variance: {explained_variance}\")\n",
        "print(f\"Total variance explained by 2 components: {explained_variance.sum():.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Choosing the Number of Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze how many components we need for Wine dataset\n",
        "pca_full = PCA()\n",
        "X_pca_full = pca_full.fit_transform(X_wine_scaled)\n",
        "\n",
        "# Calculate cumulative explained variance\n",
        "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "# Find number of components for 95% variance\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "n_components_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Individual explained variance\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
        "        pca_full.explained_variance_ratio_)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Individual Explained Variance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Cumulative explained variance\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
        "plt.axhline(y=0.99, color='g', linestyle='--', label='99% variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Scree plot (eigenvalues)\n",
        "plt.subplot(1, 3, 3)\n",
        "eigenvalues = pca_full.explained_variance_\n",
        "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'ro-')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Eigenvalue')\n",
        "plt.title('Scree Plot (Eigenvalues)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
        "print(f\"Components needed for 99% variance: {n_components_99}\")\n",
        "print(f\"Total variance with 2 components: {cumulative_variance[1]:.1%}\")\n",
        "print(f\"Total variance with 3 components: {cumulative_variance[2]:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. PCA vs t-SNE Comparison {#comparison}\n",
        "\n",
        "### 6.1 Side-by-Side Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive comparison table\n",
        "comparison_data = {\n",
        "    'Aspect': [\n",
        "        'Type', 'Linear/Non-linear', 'Reversible', 'Computational Speed', \n",
        "        'Memory Usage', 'Global Structure', 'Local Structure', 'Interpretability',\n",
        "        'Stability', 'Use Case'\n",
        "    ],\n",
        "    'PCA': [\n",
        "        'Linear', 'Linear', 'Yes', 'Fast', 'Low', 'Preserves', 'May lose', \n",
        "        'High (principal components)', 'Stable', 'Feature reduction, noise removal'\n",
        "    ],\n",
        "    't-SNE': [\n",
        "        'Non-linear', 'Non-linear', 'No', 'Slow', 'High', 'May lose', \n",
        "        'Preserves', 'Low (black box)', 'Variable (random)', 'Visualization, clustering'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"ðŸ“Š PCA vs t-SNE Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visual comparison with different datasets\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Iris dataset\n",
        "axes[0, 0].scatter(X_pca_iris[:, 0], X_pca_iris[:, 1], c=y_iris, alpha=0.7)\n",
        "axes[0, 0].set_title('PCA - Iris Dataset')\n",
        "axes[0, 0].set_xlabel('PC1')\n",
        "axes[0, 0].set_ylabel('PC2')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].scatter(X_tsne_iris[:, 0], X_tsne_iris[:, 1], c=y_iris, alpha=0.7)\n",
        "axes[0, 1].set_title('t-SNE - Iris Dataset')\n",
        "axes[0, 1].set_xlabel('t-SNE 1')\n",
        "axes[0, 1].set_ylabel('t-SNE 2')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Wine dataset\n",
        "axes[0, 2].scatter(X_pca_wine[:, 0], X_pca_wine[:, 1], c=y_wine, alpha=0.7)\n",
        "axes[0, 2].set_title('PCA - Wine Dataset')\n",
        "axes[0, 2].set_xlabel('PC1')\n",
        "axes[0, 2].set_ylabel('PC2')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].scatter(X_tsne_wine[:, 0], X_tsne_wine[:, 1], c=y_wine, alpha=0.7)\n",
        "axes[1, 0].set_title('t-SNE - Wine Dataset')\n",
        "axes[1, 0].set_xlabel('t-SNE 1')\n",
        "axes[1, 0].set_ylabel('t-SNE 2')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Performance comparison\n",
        "axes[1, 1].bar(['PCA', 't-SNE'], [0.001, 2.5], color=['blue', 'red'], alpha=0.7)\n",
        "axes[1, 1].set_title('Computational Speed (seconds)')\n",
        "axes[1, 1].set_ylabel('Time (seconds)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory usage\n",
        "axes[1, 2].bar(['PCA', 't-SNE'], [1, 5], color=['blue', 'red'], alpha=0.7)\n",
        "axes[1, 2].set_title('Memory Usage (relative)')\n",
        "axes[1, 2].set_ylabel('Memory (relative)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Advanced Techniques {#advanced}\n",
        "\n",
        "### 7.1 PCA for Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use PCA for feature engineering and compare performance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_wine_scaled, y_wine, \n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# Train models with different numbers of PCA components\n",
        "n_components_list = [2, 5, 10, 13]  # 13 = all features\n",
        "results = []\n",
        "\n",
        "for n_comp in n_components_list:\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "    \n",
        "    # Train Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    scores = cross_val_score(rf, X_train_pca, y_train, cv=5)\n",
        "    \n",
        "    results.append({\n",
        "        'n_components': n_comp,\n",
        "        'mean_score': scores.mean(),\n",
        "        'std_score': scores.std(),\n",
        "        'explained_variance': pca.explained_variance_ratio_.sum()\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"ðŸŽ¯ PCA Feature Engineering Results:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualize results\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Accuracy vs components\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.errorbar(results_df['n_components'], results_df['mean_score'], \n",
        "             yerr=results_df['std_score'], marker='o', capsize=5)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('Model Performance vs PCA Components')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Explained variance vs components\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(results_df['n_components'], results_df['explained_variance'], 'ro-')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance vs Components')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy vs explained variance\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(results_df['explained_variance'], results_df['mean_score'], \n",
        "           s=100, alpha=0.7)\n",
        "for i, n_comp in enumerate(results_df['n_components']):\n",
        "    plt.annotate(f'{n_comp}', (results_df['explained_variance'].iloc[i], \n",
        "                               results_df['mean_score'].iloc[i]))\n",
        "plt.xlabel('Explained Variance Ratio')\n",
        "plt.ylabel('Cross-Validation Accuracy')\n",
        "plt.title('Accuracy vs Explained Variance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Key insights:\")\n",
        "print(f\"â€¢ Best performance with {results_df.loc[results_df['mean_score'].idxmax(), 'n_components']} components\")\n",
        "print(f\"â€¢ {results_df['explained_variance'].iloc[1]:.1%} variance with 5 components\")\n",
        "print(f\"â€¢ Performance plateaus after 5 components\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 t-SNE Parameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different perplexity values for t-SNE\n",
        "perplexity_values = [5, 15, 30, 50]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, perplexity in enumerate(perplexity_values):\n",
        "    print(f\"Running t-SNE with perplexity={perplexity}...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "    X_tsne = tsne.fit_transform(X_wine_scaled)\n",
        "    \n",
        "    # Plot results\n",
        "    for j, color in enumerate(wine_colors):\n",
        "        axes[i].scatter(X_tsne[y_wine == j, 0], X_tsne[y_wine == j, 1], \n",
        "                       c=color, label=wine.target_names[j], alpha=0.7)\n",
        "    \n",
        "    axes[i].set_title(f't-SNE with Perplexity={perplexity}')\n",
        "    axes[i].set_xlabel('t-SNE Component 1')\n",
        "    axes[i].set_ylabel('t-SNE Component 2')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ’¡ Perplexity effects:\")\n",
        "print(\"â€¢ Low perplexity (5-15): Focuses on local structure, tight clusters\")\n",
        "print(\"â€¢ Medium perplexity (30): Balanced local and global structure\")\n",
        "print(\"â€¢ High perplexity (50+): Focuses on global structure, looser clusters\")\n",
        "print(\"â€¢ Rule of thumb: perplexity should be 5-50, typically 30\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 High Dimensional Data - 3D and 2D Reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create high-dimensional synthetic data\n",
        "def create_high_dim_data(n_samples=1000, n_features=20, n_classes=4, noise=0.1):\n",
        "    \"\"\"Create high-dimensional data with clear cluster structure\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create cluster centers in high-dimensional space\n",
        "    cluster_centers = np.random.randn(n_classes, n_features) * 3\n",
        "    \n",
        "    X = []\n",
        "    y = []\n",
        "    \n",
        "    for class_id in range(n_classes):\n",
        "        # Generate samples around each cluster center\n",
        "        n_samples_per_class = n_samples // n_classes\n",
        "        class_data = np.random.randn(n_samples_per_class, n_features) * noise + cluster_centers[class_id]\n",
        "        X.append(class_data)\n",
        "        y.extend([class_id] * n_samples_per_class)\n",
        "    \n",
        "    X = np.vstack(X)\n",
        "    y = np.array(y)\n",
        "    \n",
        "    # Shuffle the data\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "# Create high-dimensional data\n",
        "print(\"Creating high-dimensional dataset...\")\n",
        "X_high_dim, y_high_dim = create_high_dim_data(n_samples=1000, n_features=20, n_classes=4)\n",
        "\n",
        "print(f\"High-dimensional dataset shape: {X_high_dim.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y_high_dim))}\")\n",
        "print(f\"Features per class: {X_high_dim.shape[0] // len(np.unique(y_high_dim))}\")\n",
        "\n",
        "# Standardize the data\n",
        "scaler_high = StandardScaler()\n",
        "X_high_dim_scaled = scaler_high.fit_transform(X_high_dim)\n",
        "\n",
        "print(\"âœ… High-dimensional data created and standardized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA to reduce to 3D and 2D\n",
        "print(\"Applying PCA to reduce dimensions...\")\n",
        "\n",
        "# PCA to 3D\n",
        "pca_3d = PCA(n_components=3)\n",
        "X_pca_3d = pca_3d.fit_transform(X_high_dim_scaled)\n",
        "\n",
        "# PCA to 2D\n",
        "pca_2d = PCA(n_components=2)\n",
        "X_pca_2d = pca_2d.fit_transform(X_high_dim_scaled)\n",
        "\n",
        "# Calculate explained variance\n",
        "explained_variance_3d = pca_3d.explained_variance_ratio_\n",
        "explained_variance_2d = pca_2d.explained_variance_ratio_\n",
        "\n",
        "print(f\"3D PCA - Explained variance: {explained_variance_3d}\")\n",
        "print(f\"3D PCA - Total variance explained: {explained_variance_3d.sum():.1%}\")\n",
        "print(f\"2D PCA - Explained variance: {explained_variance_2d}\")\n",
        "print(f\"2D PCA - Total variance explained: {explained_variance_2d.sum():.1%}\")\n",
        "\n",
        "# Visualize 3D PCA results\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(20, 6))\n",
        "\n",
        "# 3D PCA visualization\n",
        "ax1 = fig.add_subplot(131, projection='3d')\n",
        "colors_3d = ['red', 'green', 'blue', 'orange']\n",
        "for i, color in enumerate(colors_3d):\n",
        "    mask = y_high_dim == i\n",
        "    ax1.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2], \n",
        "                c=color, label=f'Class {i}', alpha=0.6, s=30)\n",
        "ax1.set_xlabel('First Principal Component')\n",
        "ax1.set_ylabel('Second Principal Component')\n",
        "ax1.set_zlabel('Third Principal Component')\n",
        "ax1.set_title('PCA: 20D â†’ 3D')\n",
        "ax1.legend()\n",
        "\n",
        "# 2D PCA visualization\n",
        "ax2 = fig.add_subplot(132)\n",
        "for i, color in enumerate(colors_3d):\n",
        "    mask = y_high_dim == i\n",
        "    ax2.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
        "                c=color, label=f'Class {i}', alpha=0.6, s=30)\n",
        "ax2.set_xlabel('First Principal Component')\n",
        "ax2.set_ylabel('Second Principal Component')\n",
        "ax2.set_title('PCA: 20D â†’ 2D')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Explained variance comparison\n",
        "ax3 = fig.add_subplot(133)\n",
        "components = ['PC1', 'PC2', 'PC3']\n",
        "variance_3d = explained_variance_3d\n",
        "variance_2d = np.append(explained_variance_2d, 0)  # Pad for comparison\n",
        "x = np.arange(len(components))\n",
        "width = 0.35\n",
        "\n",
        "ax3.bar(x - width/2, variance_3d, width, label='3D PCA', alpha=0.8)\n",
        "ax3.bar(x + width/2, variance_2d, width, label='2D PCA', alpha=0.8)\n",
        "ax3.set_xlabel('Principal Components')\n",
        "ax3.set_ylabel('Explained Variance Ratio')\n",
        "ax3.set_title('Explained Variance Comparison')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(components)\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… PCA visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply t-SNE to reduce to 2D\n",
        "print(\"Applying t-SNE to reduce 20D â†’ 2D...\")\n",
        "print(\"This may take a moment due to the high dimensionality...\")\n",
        "\n",
        "# t-SNE to 2D\n",
        "tsne_2d = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
        "X_tsne_2d = tsne_2d.fit_transform(X_high_dim_scaled)\n",
        "\n",
        "print(\"âœ… t-SNE reduction complete!\")\n",
        "\n",
        "# Compare PCA vs t-SNE for high-dimensional data\n",
        "fig = plt.figure(figsize=(20, 8))\n",
        "\n",
        "# PCA 3D\n",
        "ax1 = fig.add_subplot(221, projection='3d')\n",
        "for i, color in enumerate(colors_3d):\n",
        "    mask = y_high_dim == i\n",
        "    ax1.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2], \n",
        "                c=color, label=f'Class {i}', alpha=0.6, s=20)\n",
        "ax1.set_xlabel('PC1')\n",
        "ax1.set_ylabel('PC2')\n",
        "ax1.set_zlabel('PC3')\n",
        "ax1.set_title('PCA: 20D â†’ 3D')\n",
        "ax1.legend()\n",
        "\n",
        "# PCA 2D\n",
        "ax2 = fig.add_subplot(222)\n",
        "for i, color in enumerate(colors_3d):\n",
        "    mask = y_high_dim == i\n",
        "    ax2.scatter(X_pca_2d[mask, 0], X_pca_2d[mask, 1], \n",
        "                c=color, label=f'Class {i}', alpha=0.6, s=20)\n",
        "ax2.set_xlabel('PC1')\n",
        "ax2.set_ylabel('PC2')\n",
        "ax2.set_title('PCA: 20D â†’ 2D')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# t-SNE 2D\n",
        "ax3 = fig.add_subplot(223)\n",
        "for i, color in enumerate(colors_3d):\n",
        "    mask = y_high_dim == i\n",
        "    ax3.scatter(X_tsne_2d[mask, 0], X_tsne_2d[mask, 1], \n",
        "                c=color, label=f'Class {i}', alpha=0.6, s=20)\n",
        "ax3.set_xlabel('t-SNE 1')\n",
        "ax3.set_ylabel('t-SNE 2')\n",
        "ax3.set_title('t-SNE: 20D â†’ 2D')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Variance explained comparison\n",
        "ax4 = fig.add_subplot(224)\n",
        "dimensions = ['2D', '3D', '20D (Original)']\n",
        "variance_explained = [\n",
        "    explained_variance_2d.sum(),\n",
        "    explained_variance_3d.sum(),\n",
        "    1.0  # Original data has 100% variance\n",
        "]\n",
        "colors_bar = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "\n",
        "bars = ax4.bar(dimensions, variance_explained, color=colors_bar, alpha=0.8)\n",
        "ax4.set_ylabel('Variance Explained')\n",
        "ax4.set_title('Variance Explained by Dimension')\n",
        "ax4.set_ylim(0, 1.1)\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, value in zip(bars, variance_explained):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{value:.1%}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸŽ¯ High-dimensional reduction complete!\")\n",
        "print(f\"ðŸ“Š PCA 2D explains {explained_variance_2d.sum():.1%} of variance\")\n",
        "print(f\"ðŸ“Š PCA 3D explains {explained_variance_3d.sum():.1%} of variance\")\n",
        "print(\"ðŸ’¡ Notice how t-SNE creates tighter clusters than PCA!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Performance Impact of Dimensionality Reduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare machine learning performance with different dimensions\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "\n",
        "print(\"Testing machine learning performance with different dimensions...\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_high_dim_scaled, y_high_dim, \n",
        "                                                    test_size=0.3, random_state=42)\n",
        "\n",
        "# Test different dimensionalities\n",
        "dimensions_to_test = [2, 3, 5, 10, 20]  # 20 = original\n",
        "results = []\n",
        "\n",
        "for n_dims in dimensions_to_test:\n",
        "    print(f\"Testing with {n_dims} dimensions...\")\n",
        "    \n",
        "    # Apply PCA if reducing dimensions\n",
        "    if n_dims < 20:\n",
        "        pca = PCA(n_components=n_dims)\n",
        "        X_train_reduced = pca.fit_transform(X_train)\n",
        "        X_test_reduced = pca.transform(X_test)\n",
        "        explained_var = pca.explained_variance_ratio_.sum()\n",
        "    else:\n",
        "        X_train_reduced = X_train\n",
        "        X_test_reduced = X_test\n",
        "        explained_var = 1.0\n",
        "    \n",
        "    # Train Random Forest\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    \n",
        "    # Measure training time\n",
        "    start_time = time.time()\n",
        "    rf.fit(X_train_reduced, y_train)\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Cross-validation score\n",
        "    cv_scores = cross_val_score(rf, X_train_reduced, y_train, cv=5)\n",
        "    \n",
        "    # Test accuracy\n",
        "    test_accuracy = rf.score(X_test_reduced, y_test)\n",
        "    \n",
        "    results.append({\n",
        "        'dimensions': n_dims,\n",
        "        'explained_variance': explained_var,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'test_accuracy': test_accuracy,\n",
        "        'training_time': training_time\n",
        "    })\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\nðŸ“Š Performance Results:\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Accuracy vs dimensions\n",
        "axes[0, 0].errorbar(results_df['dimensions'], results_df['cv_mean'], \n",
        "                    yerr=results_df['cv_std'], marker='o', capsize=5, label='CV Score')\n",
        "axes[0, 0].plot(results_df['dimensions'], results_df['test_accuracy'], \n",
        "                marker='s', label='Test Accuracy')\n",
        "axes[0, 0].set_xlabel('Number of Dimensions')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Model Performance vs Dimensions')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training time vs dimensions\n",
        "axes[0, 1].plot(results_df['dimensions'], results_df['training_time'], 'ro-')\n",
        "axes[0, 1].set_xlabel('Number of Dimensions')\n",
        "axes[0, 1].set_ylabel('Training Time (seconds)')\n",
        "axes[0, 1].set_title('Training Time vs Dimensions')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Explained variance vs dimensions\n",
        "axes[1, 0].plot(results_df['dimensions'], results_df['explained_variance'], 'go-')\n",
        "axes[1, 0].set_xlabel('Number of Dimensions')\n",
        "axes[1, 0].set_ylabel('Explained Variance Ratio')\n",
        "axes[1, 0].set_title('Explained Variance vs Dimensions')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy vs explained variance\n",
        "axes[1, 1].scatter(results_df['explained_variance'], results_df['test_accuracy'], \n",
        "                   s=100, alpha=0.7)\n",
        "for i, dims in enumerate(results_df['dimensions']):\n",
        "    axes[1, 1].annotate(f'{dims}D', \n",
        "                        (results_df['explained_variance'].iloc[i], \n",
        "                         results_df['test_accuracy'].iloc[i]))\n",
        "axes[1, 1].set_xlabel('Explained Variance Ratio')\n",
        "axes[1, 1].set_ylabel('Test Accuracy')\n",
        "axes[1, 1].set_title('Accuracy vs Explained Variance')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find optimal number of dimensions\n",
        "best_idx = results_df['test_accuracy'].idxmax()\n",
        "best_dims = results_df.loc[best_idx, 'dimensions']\n",
        "best_accuracy = results_df.loc[best_idx, 'test_accuracy']\n",
        "best_variance = results_df.loc[best_idx, 'explained_variance']\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Optimal Results:\")\n",
        "print(f\"â€¢ Best performance: {best_dims} dimensions\")\n",
        "print(f\"â€¢ Test accuracy: {best_accuracy:.3f}\")\n",
        "print(f\"â€¢ Explained variance: {best_variance:.1%}\")\n",
        "print(f\"â€¢ Speed improvement: {results_df.loc[results_df['dimensions'] == 20, 'training_time'].iloc[0] / results_df.loc[best_idx, 'training_time']:.1f}x faster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 The Curse of Dimensionality Demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the curse of dimensionality\n",
        "def demonstrate_curse_of_dimensionality():\n",
        "    \"\"\"Show how data becomes sparse in high dimensions\"\"\"\n",
        "    dimensions = [1, 2, 3, 5, 10, 20, 50]\n",
        "    n_samples = 1000\n",
        "    \n",
        "    # Calculate average distance to nearest neighbor\n",
        "    avg_distances = []\n",
        "    volume_ratios = []\n",
        "    \n",
        "    for dim in dimensions:\n",
        "        # Generate random data in hypercube [0,1]^dim\n",
        "        data = np.random.rand(n_samples, dim)\n",
        "        \n",
        "        # Calculate pairwise distances\n",
        "        from sklearn.metrics.pairwise import euclidean_distances\n",
        "        distances = euclidean_distances(data)\n",
        "        \n",
        "        # Set diagonal to infinity to exclude self-distances\n",
        "        np.fill_diagonal(distances, np.inf)\n",
        "        \n",
        "        # Find minimum distance for each point (nearest neighbor)\n",
        "        min_distances = np.min(distances, axis=1)\n",
        "        avg_distances.append(np.mean(min_distances))\n",
        "        \n",
        "        # Calculate volume ratio (volume of hypersphere / volume of hypercube)\n",
        "        # For unit hypercube, volume = 1\n",
        "        # For unit hypersphere, volume = Ï€^(d/2) / Î“(d/2 + 1)\n",
        "        from scipy.special import gamma\n",
        "        sphere_volume = (np.pi ** (dim/2)) / gamma(dim/2 + 1)\n",
        "        cube_volume = 1.0\n",
        "        volume_ratios.append(sphere_volume / cube_volume)\n",
        "    \n",
        "    return dimensions, avg_distances, volume_ratios\n",
        "\n",
        "# Run the demonstration\n",
        "print(\"Demonstrating the curse of dimensionality...\")\n",
        "dims, avg_dists, vol_ratios = demonstrate_curse_of_dimensionality()\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Average distance to nearest neighbor\n",
        "axes[0, 0].plot(dims, avg_dists, 'bo-', linewidth=2, markersize=8)\n",
        "axes[0, 0].set_xlabel('Number of Dimensions')\n",
        "axes[0, 0].set_ylabel('Average Distance to Nearest Neighbor')\n",
        "axes[0, 0].set_title('Curse of Dimensionality: Distance to Nearest Neighbor')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# Volume ratio\n",
        "axes[0, 1].plot(dims, vol_ratios, 'ro-', linewidth=2, markersize=8)\n",
        "axes[0, 1].set_xlabel('Number of Dimensions')\n",
        "axes[0, 1].set_ylabel('Volume Ratio (Sphere/Cube)')\n",
        "axes[0, 1].set_title('Volume Ratio in High Dimensions')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_yscale('log')\n",
        "\n",
        "# Data sparsity visualization\n",
        "axes[1, 0].bar(range(len(dims)), [1/d for d in dims], alpha=0.7, color='green')\n",
        "axes[1, 0].set_xlabel('Number of Dimensions')\n",
        "axes[1, 0].set_ylabel('Data Density (1/dimensions)')\n",
        "axes[1, 0].set_title('Data Density Decreases with Dimensions')\n",
        "axes[1, 0].set_xticks(range(len(dims)))\n",
        "axes[1, 0].set_xticklabels(dims)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distance distribution in different dimensions\n",
        "axes[1, 1].hist(avg_dists, bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Average Distance to Nearest Neighbor')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Distribution of Nearest Neighbor Distances')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print insights\n",
        "print(\"\\nðŸ“Š Curse of Dimensionality Insights:\")\n",
        "print(f\"â€¢ 1D: Average distance = {avg_dists[0]:.3f}\")\n",
        "print(f\"â€¢ 2D: Average distance = {avg_dists[1]:.3f}\")\n",
        "print(f\"â€¢ 10D: Average distance = {avg_dists[4]:.3f}\")\n",
        "print(f\"â€¢ 20D: Average distance = {avg_dists[5]:.3f}\")\n",
        "print(f\"â€¢ 50D: Average distance = {avg_dists[6]:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Volume Ratio Insights:\")\n",
        "print(f\"â€¢ 1D: Volume ratio = {vol_ratios[0]:.3f}\")\n",
        "print(f\"â€¢ 2D: Volume ratio = {vol_ratios[1]:.3f}\")\n",
        "print(f\"â€¢ 10D: Volume ratio = {vol_ratios[4]:.6f}\")\n",
        "print(f\"â€¢ 20D: Volume ratio = {vol_ratios[5]:.2e}\")\n",
        "print(f\"â€¢ 50D: Volume ratio = {vol_ratios[6]:.2e}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Key Takeaways:\")\n",
        "print(\"â€¢ In high dimensions, all points become equidistant\")\n",
        "print(\"â€¢ Volume concentrates in the corners of the hypercube\")\n",
        "print(\"â€¢ Distance metrics become less meaningful\")\n",
        "print(\"â€¢ This is why dimensionality reduction is so important!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Summary and Key Takeaways {#summary}\n",
        "\n",
        "### ðŸŽ¯ When to Use Each Method:\n",
        "\n",
        "#### **Use PCA when:**\n",
        "- **Linear relationships** exist in your data\n",
        "- **Feature reduction** is the main goal\n",
        "- **Interpretability** is important\n",
        "- **Computational speed** matters\n",
        "- **Reversibility** is needed\n",
        "- **Global structure** preservation is important\n",
        "\n",
        "#### **Use t-SNE when:**\n",
        "- **Visualization** is the primary goal\n",
        "- **Non-linear patterns** exist in your data\n",
        "- **Local structure** preservation is important\n",
        "- **Clustering** and pattern discovery\n",
        "- **Exploratory data analysis**\n",
        "- **Complex relationships** need to be revealed\n",
        "\n",
        "### ðŸ“Š Key Insights from Our Examples:\n",
        "\n",
        "#### **PCA Results:**\n",
        "- **Iris Dataset**: 97.8% variance explained with 2 components\n",
        "- **Wine Dataset**: 55.4% variance explained with 2 components\n",
        "- **Linear relationships** preserved well\n",
        "- **Fast computation** and **stable results**\n",
        "\n",
        "#### **t-SNE Results:**\n",
        "- **Tighter clusters** than PCA\n",
        "- **Better separation** of classes\n",
        "- **Non-linear patterns** revealed\n",
        "- **Slower computation** but **better visualization**\n",
        "\n",
        "### ðŸ”§ Best Practices:\n",
        "\n",
        "#### **Data Preprocessing:**\n",
        "1. **Standardize** your data before PCA\n",
        "2. **Handle missing values** appropriately\n",
        "3. **Remove outliers** if they dominate the analysis\n",
        "4. **Consider scaling** for t-SNE (though less critical)\n",
        "\n",
        "#### **Parameter Selection:**\n",
        "1. **PCA**: Choose components based on explained variance (95-99%)\n",
        "2. **t-SNE**: Start with perplexity=30, adjust based on data size\n",
        "3. **Iterations**: Use 1000+ for t-SNE convergence\n",
        "4. **Random state**: Set for reproducible results\n",
        "\n",
        "#### **Interpretation:**\n",
        "1. **PCA components** have clear mathematical meaning\n",
        "2. **t-SNE results** are for visualization only\n",
        "3. **Don't over-interpret** t-SNE distances\n",
        "4. **Use multiple runs** for t-SNE to check stability\n",
        "\n",
        "### ðŸš€ Advanced Applications:\n",
        "\n",
        "#### **PCA Applications:**\n",
        "- **Noise reduction** in images\n",
        "- **Feature engineering** for ML models\n",
        "- **Data compression** and storage\n",
        "- **Anomaly detection** (reconstruction error)\n",
        "- **Signal processing** and analysis\n",
        "\n",
        "#### **t-SNE Applications:**\n",
        "- **Gene expression** analysis\n",
        "- **Image similarity** visualization\n",
        "- **Text analysis** and topic modeling\n",
        "- **Customer segmentation** visualization\n",
        "- **Neural network** hidden layer visualization\n",
        "\n",
        "### âš ï¸ Common Pitfalls:\n",
        "\n",
        "#### **PCA Pitfalls:**\n",
        "- **Not standardizing** data before PCA\n",
        "- **Over-interpreting** principal components\n",
        "- **Assuming linearity** when data is non-linear\n",
        "- **Ignoring** the curse of dimensionality\n",
        "\n",
        "#### **t-SNE Pitfalls:**\n",
        "- **Interpreting distances** as meaningful\n",
        "- **Using results** for anything other than visualization\n",
        "- **Ignoring** the stochastic nature\n",
        "- **Not tuning** perplexity parameter\n",
        "\n",
        "### ðŸŽ“ Next Steps:\n",
        "\n",
        "1. **Explore UMAP** - Modern alternative to t-SNE\n",
        "2. **Try LDA** - Supervised dimensionality reduction\n",
        "3. **Experiment with** different datasets\n",
        "4. **Combine methods** - PCA + t-SNE pipeline\n",
        "5. **Apply to** your own domain-specific data\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ† Congratulations!\n",
        "\n",
        "You've learned the fundamentals of dimensionality reduction with PCA and t-SNE! These powerful techniques will help you:\n",
        "\n",
        "- **Visualize** high-dimensional data\n",
        "- **Reduce** computational complexity\n",
        "- **Discover** hidden patterns\n",
        "- **Improve** machine learning models\n",
        "- **Communicate** data insights effectively\n",
        "\n",
        "Remember: **PCA for analysis, t-SNE for visualization!** ðŸŽ¯ðŸ“Š\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
