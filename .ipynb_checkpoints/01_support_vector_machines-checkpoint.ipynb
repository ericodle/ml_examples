{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machines (SVM) - Complete Guide\n",
        "\n",
        "## Table of Contents\n",
        "1. [What is an SVM?](#what-is-svm)\n",
        "2. [How SVMs Work - The Theory](#how-svms-work)\n",
        "3. [Visual Demonstrations](#visual-demos)\n",
        "4. [SVM Variants](#svm-variants)\n",
        "5. [Practical Examples](#practical-examples)\n",
        "6. [Comparison of Different Kernels](#kernel-comparison)\n",
        "7. [Summary and Key Takeaways](#summary)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is an SVM? {#what-is-svm}\n",
        "\n",
        "**Support Vector Machine (SVM)** is a powerful supervised learning algorithm used for both classification and regression tasks. Here's what makes SVMs special:\n",
        "\n",
        "### Key Concepts:\n",
        "- **Support Vectors**: The data points closest to the decision boundary\n",
        "- **Margin**: The distance between the decision boundary and the nearest data points\n",
        "- **Hyperplane**: The decision boundary that separates different classes\n",
        "- **Kernel Trick**: A method to handle non-linearly separable data\n",
        "\n",
        "### Why SVMs are Popular:\n",
        "1. **Effective in high-dimensional spaces**\n",
        "2. **Memory efficient** (only uses support vectors)\n",
        "3. **Versatile** (can handle both linear and non-linear data)\n",
        "4. **Robust** to overfitting\n",
        "5. **Works well with small datasets**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(\"üìä Ready to explore Support Vector Machines!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How SVMs Work - The Theory {#how-svms-work}\n",
        "\n",
        "### The Mathematical Foundation\n",
        "\n",
        "SVMs work by finding the **optimal hyperplane** that separates classes with the **maximum margin**.\n",
        "\n",
        "#### For Linear Classification:\n",
        "- **Decision Function**: f(x) = w^T ¬∑ x + b\n",
        "- **Goal**: Maximize the margin while correctly classifying all points\n",
        "- **Constraint**: y_i(w^T ¬∑ x_i + b) ‚â• 1 for all i\n",
        "\n",
        "#### The Optimization Problem:\n",
        "```\n",
        "Minimize: (1/2)||w||¬≤\n",
        "Subject to: y_i(w^T ¬∑ x_i + b) ‚â• 1\n",
        "```\n",
        "\n",
        "#### Key Insight:\n",
        "Only the **support vectors** (points on the margin) matter for the final decision boundary. All other points can be removed without affecting the result!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visual Demonstrations {#visual-demos}\n",
        "\n",
        "Let's create visual demonstrations to understand SVM concepts better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple 2D dataset for visualization\n",
        "def create_sample_data():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Class 1: Blue points\n",
        "    X1 = np.random.randn(50, 2) + [2, 2]\n",
        "    y1 = np.ones(50)\n",
        "    \n",
        "    # Class 2: Red points  \n",
        "    X2 = np.random.randn(50, 2) + [-2, -2]\n",
        "    y2 = -np.ones(50)\n",
        "    \n",
        "    X = np.vstack([X1, X2])\n",
        "    y = np.hstack([y1, y2])\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X, y = create_sample_data()\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the dataset\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Plot the data points\n",
        "colors = ['red' if label == -1 else 'blue' for label in y]\n",
        "plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.7, s=100, edgecolors='black')\n",
        "\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('Sample Dataset for SVM Demonstration', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add legend\n",
        "plt.scatter([], [], c='red', label='Class -1', s=100)\n",
        "plt.scatter([], [], c='blue', label='Class +1', s=100)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train a linear SVM and visualize the decision boundary\n",
        "def plot_svm_decision_boundary(X, y, model, title=\"SVM Decision Boundary\"):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Create a mesh grid\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    # Make predictions on the mesh grid\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot the decision boundary and margins\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "    \n",
        "    # Plot the data points\n",
        "    colors = ['red' if label == -1 else 'blue' for label in y]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8, s=100, edgecolors='black')\n",
        "    \n",
        "    # Highlight support vectors\n",
        "    support_vectors = model.support_vectors_\n",
        "    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
        "               s=200, facecolors='none', edgecolors='black', linewidth=2, \n",
        "               label='Support Vectors')\n",
        "    \n",
        "    plt.xlabel('Feature 1', fontsize=12)\n",
        "    plt.ylabel('Feature 2', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Train linear SVM\n",
        "svm_linear = SVC(kernel='linear', C=1.0)\n",
        "svm_linear.fit(X, y)\n",
        "\n",
        "print(f\"Number of support vectors: {len(svm_linear.support_vectors_)}\")\n",
        "print(f\"Support vector indices: {svm_linear.support_}\")\n",
        "print(f\"Model accuracy: {svm_linear.score(X, y):.3f}\")\n",
        "\n",
        "plot_svm_decision_boundary(X, y, svm_linear, \"Linear SVM with Support Vectors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Margin\n",
        "\n",
        "The **margin** is the distance between the decision boundary and the nearest data points. SVMs try to maximize this margin because:\n",
        "\n",
        "1. **Larger margins** ‚Üí More confident predictions\n",
        "2. **Better generalization** ‚Üí Less likely to overfit\n",
        "3. **Robust to noise** ‚Üí Small changes in data won't affect the boundary much\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the effect of different C values (regularization parameter)\n",
        "def compare_c_values(X, y):\n",
        "    C_values = [0.1, 1, 10, 100]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, C in enumerate(C_values):\n",
        "        # Train SVM with different C values\n",
        "        svm = SVC(kernel='linear', C=C)\n",
        "        svm.fit(X, y)\n",
        "        \n",
        "        # Create mesh grid\n",
        "        h = 0.02\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                             np.arange(y_min, y_max, h))\n",
        "        \n",
        "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "        colors = ['red' if label == -1 else 'blue' for label in y]\n",
        "        axes[i].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8, s=60, edgecolors='black')\n",
        "        \n",
        "        # Highlight support vectors\n",
        "        support_vectors = svm.support_vectors_\n",
        "        axes[i].scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
        "                       s=150, facecolors='none', edgecolors='black', linewidth=2)\n",
        "        \n",
        "        axes[i].set_title(f'C = {C}\\nSupport Vectors: {len(support_vectors)}', fontweight='bold')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"üîç Effect of C parameter on SVM:\")\n",
        "print(\"‚Ä¢ C = 0.1: Large margin, more regularization\")\n",
        "print(\"‚Ä¢ C = 1: Balanced margin and fit\")\n",
        "print(\"‚Ä¢ C = 10: Smaller margin, less regularization\")\n",
        "print(\"‚Ä¢ C = 100: Very small margin, minimal regularization\")\n",
        "\n",
        "compare_c_values(X, y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. SVM Variants {#svm-variants}\n",
        "\n",
        "SVMs can handle different types of data using various **kernels**. The kernel trick allows SVMs to work in higher-dimensional spaces without explicitly computing the transformation.\n",
        "\n",
        "### Common Kernel Types:\n",
        "\n",
        "1. **Linear Kernel**: K(x, y) = x^T ¬∑ y\n",
        "2. **Polynomial Kernel**: K(x, y) = (Œ≥(x^T ¬∑ y) + r)^d\n",
        "3. **RBF (Gaussian) Kernel**: K(x, y) = exp(-Œ≥||x - y||¬≤)\n",
        "4. **Sigmoid Kernel**: K(x, y) = tanh(Œ≥(x^T ¬∑ y) + r)\n",
        "\n",
        "Let's create a non-linearly separable dataset to demonstrate different kernels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a non-linearly separable dataset (XOR problem)\n",
        "def create_xor_data():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Create XOR pattern\n",
        "    X1 = np.random.randn(50, 2) + [2, 2]  # Top right\n",
        "    X2 = np.random.randn(50, 2) + [-2, -2]  # Bottom left\n",
        "    X3 = np.random.randn(50, 2) + [2, -2]  # Bottom right\n",
        "    X4 = np.random.randn(50, 2) + [-2, 2]  # Top left\n",
        "    \n",
        "    # Combine and create labels\n",
        "    X = np.vstack([X1, X2, X3, X4])\n",
        "    y = np.hstack([np.ones(50), np.ones(50), -np.ones(50), -np.ones(50)])\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_xor, y_xor = create_xor_data()\n",
        "\n",
        "# Visualize XOR dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['red' if label == -1 else 'blue' for label in y_xor]\n",
        "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, alpha=0.7, s=100, edgecolors='black')\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('XOR Dataset - Non-linearly Separable', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ This dataset is NOT linearly separable!\")\n",
        "print(\"üí° We need non-linear kernels to solve this problem.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different kernels on the XOR dataset\n",
        "def compare_kernels(X, y):\n",
        "    kernels = {\n",
        "        'Linear': SVC(kernel='linear', C=1.0),\n",
        "        'Polynomial (degree=3)': SVC(kernel='poly', degree=3, C=1.0),\n",
        "        'RBF (Gaussian)': SVC(kernel='rbf', C=1.0, gamma='scale'),\n",
        "        'Sigmoid': SVC(kernel='sigmoid', C=1.0, gamma='scale')\n",
        "    }\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, (name, model) in enumerate(kernels.items()):\n",
        "        # Train the model\n",
        "        model.fit(X, y)\n",
        "        accuracy = model.score(X, y)\n",
        "        \n",
        "        # Create mesh grid\n",
        "        h = 0.02\n",
        "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                             np.arange(y_min, y_max, h))\n",
        "        \n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
        "        colors = ['red' if label == -1 else 'blue' for label in y]\n",
        "        axes[i].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8, s=60, edgecolors='black')\n",
        "        \n",
        "        # Highlight support vectors\n",
        "        support_vectors = model.support_vectors_\n",
        "        axes[i].scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
        "                       s=150, facecolors='none', edgecolors='black', linewidth=2)\n",
        "        \n",
        "        axes[i].set_title(f'{name}\\nAccuracy: {accuracy:.3f}\\nSupport Vectors: {len(support_vectors)}', \n",
        "                         fontweight='bold')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"üîç Kernel Comparison Results:\")\n",
        "print(\"‚Ä¢ Linear: Struggles with non-linear data\")\n",
        "print(\"‚Ä¢ Polynomial: Can handle some non-linear patterns\")\n",
        "print(\"‚Ä¢ RBF: Excellent for complex non-linear boundaries\")\n",
        "print(\"‚Ä¢ Sigmoid: Similar to neural networks, but less common\")\n",
        "\n",
        "compare_kernels(X_xor, y_xor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Examples {#practical-examples}\n",
        "\n",
        "Let's work with a real dataset to see SVMs in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the famous Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data[:, :2]  # Use only first 2 features for visualization\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"üå∏ Iris Dataset Information:\")\n",
        "print(f\"Features: {iris.feature_names[:2]}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {X_iris.shape[1]}\")\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['red', 'green', 'blue']\n",
        "for i, color in enumerate(colors):\n",
        "    mask = y_iris == i\n",
        "    plt.scatter(X_iris[mask, 0], X_iris[mask, 1], \n",
        "               c=color, label=iris.target_names[i], alpha=0.7, s=100)\n",
        "\n",
        "plt.xlabel(iris.feature_names[0], fontsize=12)\n",
        "plt.ylabel(iris.feature_names[1], fontsize=12)\n",
        "plt.title('Iris Dataset - First Two Features', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train different SVM models on Iris dataset\n",
        "def train_and_evaluate_svm(X, y, kernel='rbf', C=1.0, gamma='scale'):\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Train SVM\n",
        "    svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
        "    svm.fit(X_train_scaled, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = svm.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    return svm, accuracy, y_test, y_pred, X_train_scaled, y_train\n",
        "\n",
        "# Test different kernels\n",
        "kernels_to_test = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "results = {}\n",
        "\n",
        "print(\"üöÄ Training SVMs with different kernels on Iris dataset...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for kernel in kernels_to_test:\n",
        "    svm, accuracy, y_test, y_pred, X_train_scaled, y_train = train_and_evaluate_svm(\n",
        "        X_iris, y_iris, kernel=kernel\n",
        "    )\n",
        "    results[kernel] = {\n",
        "        'model': svm,\n",
        "        'accuracy': accuracy,\n",
        "        'support_vectors': len(svm.support_vectors_)\n",
        "    }\n",
        "    print(f\"{kernel.upper():>10} Kernel: Accuracy = {accuracy:.3f}, Support Vectors = {len(svm.support_vectors_)}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decision boundaries for different kernels\n",
        "def plot_iris_decision_boundaries(X, y, results):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Scale the data for visualization\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    for i, (kernel, result) in enumerate(results.items()):\n",
        "        model = result['model']\n",
        "        accuracy = result['accuracy']\n",
        "        \n",
        "        # Create mesh grid\n",
        "        h = 0.02\n",
        "        x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
        "        y_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\n",
        "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                             np.arange(y_min, y_max, h))\n",
        "        \n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot decision boundary\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "        \n",
        "        # Plot data points\n",
        "        colors = ['red', 'green', 'blue']\n",
        "        for j, color in enumerate(colors):\n",
        "            mask = y == j\n",
        "            axes[i].scatter(X_scaled[mask, 0], X_scaled[mask, 1], \n",
        "                           c=color, label=iris.target_names[j], alpha=0.8, s=60)\n",
        "        \n",
        "        # Highlight support vectors\n",
        "        support_vectors = model.support_vectors_\n",
        "        axes[i].scatter(support_vectors[:, 0], support_vectors[:, 1], \n",
        "                       s=150, facecolors='none', edgecolors='black', linewidth=2,\n",
        "                       label='Support Vectors')\n",
        "        \n",
        "        axes[i].set_title(f'{kernel.upper()} Kernel\\nAccuracy: {accuracy:.3f}', fontweight='bold')\n",
        "        axes[i].legend(fontsize=8)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_iris_decision_boundaries(X_iris, y_iris, results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison of Different Kernels {#kernel-comparison}\n",
        "\n",
        "Let's create a comprehensive comparison table and analysis:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a detailed comparison\n",
        "import pandas as pd\n",
        "\n",
        "comparison_data = []\n",
        "for kernel, result in results.items():\n",
        "    comparison_data.append({\n",
        "        'Kernel': kernel.upper(),\n",
        "        'Accuracy': f\"{result['accuracy']:.3f}\",\n",
        "        'Support Vectors': result['support_vectors'],\n",
        "        'Best For': {\n",
        "            'linear': 'Linearly separable data, high-dimensional data',\n",
        "            'poly': 'Moderate non-linearity, interpretable features',\n",
        "            'rbf': 'Complex non-linear patterns, general purpose',\n",
        "            'sigmoid': 'Neural network-like behavior, specific cases'\n",
        "        }[kernel],\n",
        "        'Parameters': {\n",
        "            'linear': 'C (regularization)',\n",
        "            'poly': 'C, degree, gamma, coef0',\n",
        "            'rbf': 'C, gamma',\n",
        "            'sigmoid': 'C, gamma, coef0'\n",
        "        }[kernel]\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(\"üìä SVM Kernel Comparison:\")\n",
        "print(\"=\" * 80)\n",
        "print(df_comparison.to_string(index=False))\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "kernels = list(results.keys())\n",
        "accuracies = [results[k]['accuracy'] for k in kernels]\n",
        "support_vectors = [results[k]['support_vectors'] for k in kernels]\n",
        "\n",
        "ax1.bar(kernels, accuracies, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
        "ax1.set_title('Accuracy Comparison', fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0, 1)\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Support vectors comparison\n",
        "ax2.bar(kernels, support_vectors, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
        "ax2.set_title('Support Vectors Count', fontweight='bold')\n",
        "ax2.set_ylabel('Number of Support Vectors')\n",
        "for i, v in enumerate(support_vectors):\n",
        "    ax2.text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Key Takeaways {#summary}\n",
        "\n",
        "### üéØ What We Learned:\n",
        "\n",
        "1. **SVM Fundamentals**:\n",
        "   - SVMs find the optimal hyperplane with maximum margin\n",
        "   - Only support vectors matter for the final decision\n",
        "   - The C parameter controls the trade-off between margin and misclassification\n",
        "\n",
        "2. **Kernel Trick**:\n",
        "   - Allows SVMs to handle non-linear data\n",
        "   - Maps data to higher-dimensional space implicitly\n",
        "   - Different kernels work better for different data types\n",
        "\n",
        "3. **Practical Insights**:\n",
        "   - RBF kernel is often the best default choice\n",
        "   - Linear kernel is great for high-dimensional data\n",
        "   - Feature scaling is important for most kernels\n",
        "   - SVMs work well with small to medium datasets\n",
        "\n",
        "### üöÄ When to Use SVMs:\n",
        "\n",
        "‚úÖ **Good for**:\n",
        "- High-dimensional data\n",
        "- Small to medium datasets\n",
        "- Non-linear classification problems\n",
        "- When you need a robust, interpretable model\n",
        "\n",
        "‚ùå **Not ideal for**:\n",
        "- Very large datasets (slow training)\n",
        "- Noisy data with many mislabeled examples\n",
        "- When you need probability estimates\n",
        "- Text classification (other methods often work better)\n",
        "\n",
        "### üîß Key Parameters to Tune:\n",
        "\n",
        "1. **C**: Regularization parameter (higher = less regularization)\n",
        "2. **gamma**: Kernel coefficient (higher = more complex boundaries)\n",
        "3. **kernel**: Type of kernel function\n",
        "4. **degree**: For polynomial kernel\n",
        "\n",
        "### üìö Next Steps:\n",
        "\n",
        "1. Try SVMs on your own datasets\n",
        "2. Experiment with parameter tuning\n",
        "3. Compare with other algorithms (Random Forest, Neural Networks)\n",
        "4. Explore advanced topics like SVR (Support Vector Regression)\n",
        "5. Learn about multi-class SVM strategies\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! üéâ You now understand Support Vector Machines!**\n",
        "\n",
        "SVMs are a powerful tool in your machine learning toolkit. They're particularly valuable when you need a robust, interpretable model that can handle both linear and non-linear problems effectively.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
