{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Nearest Neighbors (KNN) - Complete Guide\n",
        "\n",
        "## Table of Contents\n",
        "1. [What is K-Nearest Neighbors?](#what-is-knn)\n",
        "2. [How KNN Works - The Theory](#how-knn-works)\n",
        "3. [Visual Demonstrations](#visual-demos)\n",
        "4. [Distance Metrics and Variants](#distance-metrics)\n",
        "5. [Practical Examples](#practical-examples)\n",
        "6. [Performance Analysis](#performance-analysis)\n",
        "7. [Summary and Key Takeaways](#summary)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. What is K-Nearest Neighbors? {#what-is-knn}\n",
        "\n",
        "**K-Nearest Neighbors (KNN)** is one of the simplest and most intuitive machine learning algorithms. It's a **lazy learning** algorithm that makes predictions based on the similarity to training examples.\n",
        "\n",
        "### Key Concepts:\n",
        "- **K**: Number of nearest neighbors to consider\n",
        "- **Distance Metric**: How to measure similarity between points\n",
        "- **Lazy Learning**: No explicit training phase - learning happens during prediction\n",
        "- **Instance-Based**: Stores all training data for predictions\n",
        "\n",
        "### Why KNN is Popular:\n",
        "1. **Simple to understand** and implement\n",
        "2. **No assumptions** about data distribution\n",
        "3. **Works well** with non-linear data\n",
        "4. **Versatile** - can be used for both classification and regression\n",
        "5. **No training time** - learning happens during prediction\n",
        "\n",
        "### Key Characteristics:\n",
        "- **Memory intensive** - stores all training data\n",
        "- **Slow prediction** - must compute distances to all training points\n",
        "- **Sensitive to irrelevant features** - all features are treated equally\n",
        "- **Curse of dimensionality** - performance degrades with high dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(\"üìä Ready to explore K-Nearest Neighbors!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. How KNN Works - The Theory {#how-knn-works}\n",
        "\n",
        "### The Algorithm Steps:\n",
        "\n",
        "1. **Store all training data** (no learning phase)\n",
        "2. **For a new prediction**:\n",
        "   - Calculate distance to all training points\n",
        "   - Find the K nearest neighbors\n",
        "   - For classification: majority vote\n",
        "   - For regression: average of neighbors' values\n",
        "\n",
        "### Mathematical Foundation:\n",
        "\n",
        "#### Distance Metrics:\n",
        "- **Euclidean**: d = ‚àö(Œ£(xi - yi)¬≤)\n",
        "- **Manhattan**: d = Œ£|xi - yi|\n",
        "- **Minkowski**: d = (Œ£|xi - yi|^p)^(1/p)\n",
        "- **Cosine**: d = 1 - (x¬∑y)/(||x||¬∑||y||)\n",
        "\n",
        "#### Classification:\n",
        "```\n",
        "Prediction = mode(neighbors_labels)\n",
        "```\n",
        "\n",
        "#### Regression:\n",
        "```\n",
        "Prediction = mean(neighbors_values)\n",
        "```\n",
        "\n",
        "### Key Parameters:\n",
        "- **K**: Number of neighbors (usually odd for classification)\n",
        "- **Distance metric**: How to measure similarity\n",
        "- **Weights**: Uniform or distance-based weighting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Visual Demonstrations {#visual-demos}\n",
        "\n",
        "Let's create visual demonstrations to understand KNN concepts better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a 2D dataset for KNN visualization\n",
        "def create_knn_dataset():\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Class 1: Blue points\n",
        "    X1 = np.random.randn(30, 2) + [2, 2]\n",
        "    y1 = np.ones(30)\n",
        "    \n",
        "    # Class 2: Red points  \n",
        "    X2 = np.random.randn(30, 2) + [-2, -2]\n",
        "    y2 = -np.ones(30)\n",
        "    \n",
        "    # Class 3: Green points\n",
        "    X3 = np.random.randn(30, 2) + [2, -2]\n",
        "    y3 = np.zeros(30)\n",
        "    \n",
        "    X = np.vstack([X1, X2, X3])\n",
        "    y = np.hstack([y1, y2, y3])\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_knn, y_knn = create_knn_dataset()\n",
        "print(f\"Dataset shape: {X_knn.shape}\")\n",
        "print(f\"Classes: {np.unique(y_knn)}\")\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.figure(figsize=(10, 8))\n",
        "colors = ['red' if label == -1 else 'blue' if label == 1 else 'green' for label in y_knn]\n",
        "plt.scatter(X_knn[:, 0], X_knn[:, 1], c=colors, alpha=0.7, s=100, edgecolors='black')\n",
        "\n",
        "plt.xlabel('Feature 1', fontsize=12)\n",
        "plt.ylabel('Feature 2', fontsize=12)\n",
        "plt.title('Sample Dataset for KNN Demonstration', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add legend\n",
        "plt.scatter([], [], c='red', label='Class -1', s=100)\n",
        "plt.scatter([], [], c='blue', label='Class +1', s=100)\n",
        "plt.scatter([], [], c='green', label='Class 0', s=100)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize KNN decision boundaries for different K values\n",
        "def plot_knn_decision_boundary(X, y, k_values=[1, 3, 5, 15]):\n",
        "    \"\"\"Plot KNN decision boundaries for different K values\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Create a mesh grid\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    for i, k in enumerate(k_values):\n",
        "        # Train KNN\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn.fit(X, y)\n",
        "        \n",
        "        # Make predictions on the mesh grid\n",
        "        Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot decision boundary\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "        \n",
        "        # Plot data points\n",
        "        colors = ['red' if label == -1 else 'blue' if label == 1 else 'green' for label in y]\n",
        "        axes[i].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8, s=60, edgecolors='black')\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        accuracy = knn.score(X, y)\n",
        "        \n",
        "        axes[i].set_title(f'K = {k}\\nAccuracy: {accuracy:.3f}', fontweight='bold')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"üîç Effect of K parameter on KNN:\")\n",
        "print(\"‚Ä¢ K = 1: Very sensitive to noise, overfitting\")\n",
        "print(\"‚Ä¢ K = 3: Balanced, good for most cases\")\n",
        "print(\"‚Ä¢ K = 5: Smoother boundaries, less noise sensitive\")\n",
        "print(\"‚Ä¢ K = 15: Very smooth, may underfit\")\n",
        "\n",
        "plot_knn_decision_boundary(X_knn, y_knn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate how KNN finds nearest neighbors\n",
        "def demonstrate_nearest_neighbors(X, y, test_point, k=5):\n",
        "    \"\"\"Show how KNN finds and uses nearest neighbors\"\"\"\n",
        "    # Train KNN\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X, y)\n",
        "    \n",
        "    # Find nearest neighbors\n",
        "    distances, indices = knn.kneighbors([test_point])\n",
        "    \n",
        "    # Create visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot 1: All data points\n",
        "    plt.subplot(1, 2, 1)\n",
        "    colors = ['red' if label == -1 else 'blue' if label == 1 else 'green' for label in y]\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6, s=80, edgecolors='black')\n",
        "    \n",
        "    # Highlight test point\n",
        "    plt.scatter(test_point[0], test_point[1], c='black', s=200, marker='x', linewidth=3, label='Test Point')\n",
        "    \n",
        "    # Highlight nearest neighbors\n",
        "    neighbors = X[indices[0]]\n",
        "    neighbor_labels = y[indices[0]]\n",
        "    neighbor_colors = ['red' if label == -1 else 'blue' if label == 1 else 'green' for label in neighbor_labels]\n",
        "    plt.scatter(neighbors[:, 0], neighbors[:, 1], c=neighbor_colors, s=150, edgecolors='yellow', linewidth=2, label=f'K={k} Neighbors')\n",
        "    \n",
        "    # Draw circles around neighbors\n",
        "    for i, (x, y) in enumerate(neighbors):\n",
        "        circle = plt.Circle((x, y), distances[0][i], fill=False, color='yellow', alpha=0.5, linewidth=2)\n",
        "        plt.gca().add_patch(circle)\n",
        "    \n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('KNN: Finding Nearest Neighbors')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Distance distribution\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(range(1, k+1), distances[0], color='skyblue', alpha=0.7)\n",
        "    plt.xlabel('Neighbor Rank')\n",
        "    plt.ylabel('Distance')\n",
        "    plt.title('Distance to K Nearest Neighbors')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Show prediction\n",
        "    prediction = knn.predict([test_point])[0]\n",
        "    neighbor_votes = neighbor_labels\n",
        "    vote_counts = {label: np.sum(neighbor_votes == label) for label in np.unique(neighbor_votes)}\n",
        "    \n",
        "    print(f\"üéØ Test Point: {test_point}\")\n",
        "    print(f\"üìä Nearest Neighbors: {k}\")\n",
        "    print(f\"üìè Distances: {distances[0]}\")\n",
        "    print(f\"üè∑Ô∏è  Neighbor Labels: {neighbor_votes}\")\n",
        "    print(f\"üó≥Ô∏è  Vote Counts: {vote_counts}\")\n",
        "    print(f\"üéâ Prediction: {prediction}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test with a specific point\n",
        "test_point = [0, 0]\n",
        "demonstrate_nearest_neighbors(X_knn, y_knn, test_point, k=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Distance Metrics and Variants {#distance-metrics}\n",
        "\n",
        "KNN's performance heavily depends on the distance metric used. Let's explore different options:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different distance metrics\n",
        "def compare_distance_metrics(X, y):\n",
        "    \"\"\"Compare different distance metrics for KNN\"\"\"\n",
        "    metrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\n",
        "    \n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(\"üìè Distance Metrics Comparison:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Metric':<12} {'Accuracy':<10} {'Training Time':<15}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for metric in metrics:\n",
        "        # Train KNN with different metrics\n",
        "        knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
        "        \n",
        "        # Measure training time\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        knn.fit(X_train, y_train)\n",
        "        training_time = time.time() - start_time\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        accuracy = knn.score(X_test, y_test)\n",
        "        \n",
        "        results[metric] = {\n",
        "            'accuracy': accuracy,\n",
        "            'time': training_time,\n",
        "            'model': knn\n",
        "        }\n",
        "        \n",
        "        print(f\"{metric:<12} {accuracy:<10.3f} {training_time:<15.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run comparison\n",
        "distance_results = compare_distance_metrics(X_knn, y_knn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decision boundaries for different distance metrics\n",
        "def plot_distance_metrics_decision_boundaries(X, y, results):\n",
        "    \"\"\"Plot decision boundaries for different distance metrics\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    # Create a mesh grid\n",
        "    h = 0.02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    \n",
        "    for i, (metric, result) in enumerate(results.items()):\n",
        "        model = result['model']\n",
        "        accuracy = result['accuracy']\n",
        "        \n",
        "        # Make predictions on the mesh grid\n",
        "        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        \n",
        "        # Plot decision boundary\n",
        "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "        \n",
        "        # Plot data points\n",
        "        colors = ['red' if label == -1 else 'blue' if label == 1 else 'green' for label in y]\n",
        "        axes[i].scatter(X[:, 0], X[:, 1], c=colors, alpha=0.8, s=60, edgecolors='black')\n",
        "        \n",
        "        axes[i].set_title(f'{metric.capitalize()}\\nAccuracy: {accuracy:.3f}', fontweight='bold')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_distance_metrics_decision_boundaries(X_knn, y_knn, distance_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Distance Metrics Explained:\n",
        "\n",
        "#### 1. **Euclidean Distance** (L2)\n",
        "- **Formula**: ‚àö(Œ£(xi - yi)¬≤)\n",
        "- **Best for**: Continuous features, when all features are equally important\n",
        "- **Sensitive to**: Scale differences between features\n",
        "\n",
        "#### 2. **Manhattan Distance** (L1)\n",
        "- **Formula**: Œ£|xi - yi|\n",
        "- **Best for**: Categorical features, when features are independent\n",
        "- **Less sensitive to**: Outliers compared to Euclidean\n",
        "\n",
        "#### 3. **Chebyshev Distance** (L‚àû)\n",
        "- **Formula**: max|xi - yi|\n",
        "- **Best for**: When only the maximum difference matters\n",
        "- **Use case**: Chess moves, image processing\n",
        "\n",
        "#### 4. **Minkowski Distance** (Lp)\n",
        "- **Formula**: (Œ£|xi - yi|^p)^(1/p)\n",
        "- **Generalization**: p=1 (Manhattan), p=2 (Euclidean), p=‚àû (Chebyshev)\n",
        "- **Flexible**: Can be tuned for specific applications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Practical Examples {#practical-examples}\n",
        "\n",
        "Let's work with real datasets to see KNN in action!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and work with the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"üå∏ Iris Dataset Information:\")\n",
        "print(f\"Features: {iris.feature_names}\")\n",
        "print(f\"Classes: {iris.target_names}\")\n",
        "print(f\"Samples: {X_iris.shape[0]}\")\n",
        "print(f\"Features: {X_iris.shape[1]}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale the features (important for KNN!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nüìä Data Split:\")\n",
        "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
        "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
        "print(f\"Features: {X_train_scaled.shape[1]}\")\n",
        "\n",
        "# Train KNN on Iris dataset\n",
        "knn_iris = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_iris.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn_iris.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nüéØ KNN Performance on Iris Dataset:\")\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"K value: 5\")\n",
        "print(f\"Distance metric: Euclidean (default)\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\nüìã Detailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN for Regression - Create synthetic regression data\n",
        "def demonstrate_knn_regression():\n",
        "    \"\"\"Demonstrate KNN for regression tasks\"\"\"\n",
        "    # Create regression dataset\n",
        "    X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "    \n",
        "    # Sort for better visualization\n",
        "    sort_idx = np.argsort(X_reg.ravel())\n",
        "    X_reg = X_reg[sort_idx]\n",
        "    y_reg = y_reg[sort_idx]\n",
        "    \n",
        "    # Split data\n",
        "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "        X_reg, y_reg, test_size=0.3, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Train KNN regressor\n",
        "    knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "    knn_reg.fit(X_train_reg, y_train_reg)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_reg = knn_reg.predict(X_test_reg)\n",
        "    \n",
        "    # Calculate R¬≤ score\n",
        "    from sklearn.metrics import r2_score\n",
        "    r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Plot 1: Training data and predictions\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(X_train_reg, y_train_reg, alpha=0.6, label='Training Data', color='blue')\n",
        "    plt.scatter(X_test_reg, y_test_reg, alpha=0.8, label='Test Data', color='red', s=100)\n",
        "    plt.scatter(X_test_reg, y_pred_reg, alpha=0.8, label='KNN Predictions', color='green', s=100)\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Target')\n",
        "    plt.title('KNN Regression')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Prediction vs Actual\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(y_test_reg, y_pred_reg, alpha=0.7)\n",
        "    plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(f'Prediction vs Actual (R¬≤ = {r2:.3f})')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üìà KNN Regression Results:\")\n",
        "    print(f\"R¬≤ Score: {r2:.3f}\")\n",
        "    print(f\"K value: 5\")\n",
        "    print(f\"Training samples: {len(X_train_reg)}\")\n",
        "    print(f\"Test samples: {len(X_test_reg)}\")\n",
        "\n",
        "demonstrate_knn_regression()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Analysis {#performance-analysis}\n",
        "\n",
        "Let's analyze KNN's performance characteristics and compare different approaches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the effect of K on performance\n",
        "def analyze_k_performance(X, y):\n",
        "    \"\"\"Analyze how different K values affect performance\"\"\"\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Test different K values\n",
        "    k_values = range(1, 21)\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "    \n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        knn.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        train_score = knn.score(X_train_scaled, y_train)\n",
        "        test_score = knn.score(X_test_scaled, y_test)\n",
        "        \n",
        "        train_scores.append(train_score)\n",
        "        test_scores.append(test_score)\n",
        "    \n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(k_values, train_scores, 'o-', label='Training Score', linewidth=2)\n",
        "    plt.plot(k_values, test_scores, 's-', label='Test Score', linewidth=2)\n",
        "    plt.xlabel('K Value')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('KNN Performance vs K Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Find optimal K\n",
        "    optimal_k = k_values[np.argmax(test_scores)]\n",
        "    optimal_score = max(test_scores)\n",
        "    \n",
        "    plt.axvline(x=optimal_k, color='red', linestyle='--', alpha=0.7, label=f'Optimal K = {optimal_k}')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Plot bias-variance tradeoff\n",
        "    plt.subplot(1, 2, 2)\n",
        "    bias = [1 - score for score in train_scores]\n",
        "    variance = [test - train for test, train in zip(test_scores, train_scores)]\n",
        "    \n",
        "    plt.plot(k_values, bias, 'o-', label='Bias (1 - Training Score)', linewidth=2)\n",
        "    plt.plot(k_values, variance, 's-', label='Variance (Test - Training)', linewidth=2)\n",
        "    plt.xlabel('K Value')\n",
        "    plt.ylabel('Error')\n",
        "    plt.title('Bias-Variance Tradeoff')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üéØ Performance Analysis Results:\")\n",
        "    print(f\"Optimal K: {optimal_k}\")\n",
        "    print(f\"Best Test Score: {optimal_score:.3f}\")\n",
        "    print(f\"Training Score at Optimal K: {train_scores[optimal_k-1]:.3f}\")\n",
        "    \n",
        "    return k_values, train_scores, test_scores, optimal_k\n",
        "\n",
        "# Run analysis\n",
        "k_values, train_scores, test_scores, optimal_k = analyze_k_performance(X_iris, y_iris)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation analysis\n",
        "def cross_validation_analysis(X, y):\n",
        "    \"\"\"Perform cross-validation analysis for KNN\"\"\"\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    \n",
        "    # Test different K values with cross-validation\n",
        "    k_values = range(1, 21)\n",
        "    cv_scores = []\n",
        "    \n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')\n",
        "        cv_scores.append(scores.mean())\n",
        "    \n",
        "    # Plot cross-validation results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, cv_scores, 'o-', linewidth=2, markersize=6)\n",
        "    plt.xlabel('K Value')\n",
        "    plt.ylabel('Cross-Validation Accuracy')\n",
        "    plt.title('KNN Cross-Validation Performance')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Find optimal K\n",
        "    optimal_k_cv = k_values[np.argmax(cv_scores)]\n",
        "    optimal_score_cv = max(cv_scores)\n",
        "    \n",
        "    plt.axvline(x=optimal_k_cv, color='red', linestyle='--', alpha=0.7, \n",
        "                label=f'Optimal K = {optimal_k_cv}')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"üîÑ Cross-Validation Results:\")\n",
        "    print(f\"Optimal K: {optimal_k_cv}\")\n",
        "    print(f\"Best CV Score: {optimal_score_cv:.3f}\")\n",
        "    \n",
        "    return k_values, cv_scores, optimal_k_cv\n",
        "\n",
        "# Run cross-validation analysis\n",
        "k_values_cv, cv_scores, optimal_k_cv = cross_validation_analysis(X_iris, y_iris)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Key Takeaways {#summary}\n",
        "\n",
        "### üéØ What We Learned:\n",
        "\n",
        "1. **KNN Fundamentals**:\n",
        "   - Simple, intuitive algorithm based on similarity\n",
        "   - Lazy learning - no explicit training phase\n",
        "   - Works for both classification and regression\n",
        "\n",
        "2. **Key Parameters**:\n",
        "   - **K**: Number of neighbors (affects bias-variance tradeoff)\n",
        "   - **Distance metric**: How to measure similarity\n",
        "   - **Feature scaling**: Critical for good performance\n",
        "\n",
        "3. **Performance Characteristics**:\n",
        "   - **Memory intensive**: Stores all training data\n",
        "   - **Slow prediction**: Must compute distances to all points\n",
        "   - **Sensitive to irrelevant features**: All features treated equally\n",
        "   - **Curse of dimensionality**: Performance degrades with high dimensions\n",
        "\n",
        "### üöÄ When to Use KNN:\n",
        "\n",
        "‚úÖ **Good for**:\n",
        "- Small to medium datasets\n",
        "- Non-linear decision boundaries\n",
        "- Multi-class problems\n",
        "- When you need interpretable results\n",
        "- Prototype-based learning\n",
        "\n",
        "‚ùå **Not ideal for**:\n",
        "- Large datasets (slow prediction)\n",
        "- High-dimensional data (curse of dimensionality)\n",
        "- When you need fast prediction\n",
        "- When memory is limited\n",
        "- Noisy data with many irrelevant features\n",
        "\n",
        "### üîß Key Parameters to Tune:\n",
        "\n",
        "1. **K**: Start with ‚àön (where n = number of samples)\n",
        "2. **Distance metric**: Euclidean for continuous, Manhattan for categorical\n",
        "3. **Feature scaling**: Always scale features before using KNN\n",
        "4. **Weights**: Uniform or distance-based weighting\n",
        "\n",
        "### üìö Best Practices:\n",
        "\n",
        "1. **Always scale features** - KNN is sensitive to feature scales\n",
        "2. **Use cross-validation** to find optimal K\n",
        "3. **Consider feature selection** to reduce dimensionality\n",
        "4. **Use odd K values** for classification to avoid ties\n",
        "5. **Consider computational cost** for large datasets\n",
        "\n",
        "### üÜö KNN vs Other Algorithms:\n",
        "\n",
        "| Aspect | KNN | SVM | Random Forest |\n",
        "|--------|-----|-----|---------------|\n",
        "| **Training time** | None | Fast | Medium |\n",
        "| **Prediction time** | Slow | Fast | Fast |\n",
        "| **Memory usage** | High | Low | Medium |\n",
        "| **Interpretability** | High | Medium | Medium |\n",
        "| **Non-linear data** | Excellent | Excellent | Excellent |\n",
        "| **High dimensions** | Poor | Excellent | Good |\n",
        "\n",
        "### üìà Next Steps:\n",
        "\n",
        "1. Try KNN on your own datasets\n",
        "2. Experiment with different distance metrics\n",
        "3. Compare with other algorithms\n",
        "4. Explore advanced techniques like KD-trees\n",
        "5. Learn about weighted KNN and distance weighting\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations! üéâ You now understand K-Nearest Neighbors!**\n",
        "\n",
        "KNN is a powerful, intuitive algorithm that's perfect for understanding the fundamentals of machine learning. While it has limitations with large datasets and high dimensions, it's excellent for smaller problems and provides valuable insights into similarity-based learning.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
